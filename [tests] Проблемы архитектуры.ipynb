{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбор архитектуры!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, передо мной встал вопрос о более грамотной архитектуре моего RL-кода.\n",
    "\n",
    "Основные концепции:\n",
    "* модульность - основное требование. Агент должен собираться из модулей как из кубиков лего.\n",
    "* модификация структуры алгоритма должна выражаться в виде модуля. if double_dqn: do_one_thing() else: do_another_thing() здесь не выживет.\n",
    "* в частности, у пользователя должна быть возможность в две строчки подменить какой-нибудь метод агента (например, лосс-функцию).\n",
    "* я осознаю, что в питоне можно сделать всё, но нужно адекватное, чистое и элегантное решение. Разрешается, при необходимости, спрятать требуемую мутотень в условно базовый класс, если этим будет удобно пользоваться в том числе при создании новых модулей.\n",
    "\n",
    "[внимание, код далее условный и не предназначен для запуска]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "CartpoleNN = nn.Sequential(\n",
    "                nn.Linear(4, 20),\n",
    "                nn.ELU(),\n",
    "                nn.Linear(20, 20),\n",
    "                nn.ELU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №1. Старый вариант"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Агент есть один класс.\n",
    "* Модули получаются за счёт динамического наследования друг от друга.\n",
    "* Полученный агент принимает все гиперпараметры на вход в виде конфиг-словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "# отсутствуют!\n",
    "# Возможно добавить немного костылей, чтобы делать проверку, например, что все заданные в конфиги гиперпараметры\n",
    "# действительно используются и не произошло опечатки в названии (это местная проблема)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это пишет пользователь, придумавший свою лосс-функцию\n",
    "def MyLoss(parclass):                        # соглашение: модуль может быть унаследован от произвольного класса\n",
    "    class MyLoss(parclass):        \n",
    "        def loss(self, prediction, truth):\n",
    "            return self.config[\"hp\"]         # соглашение: гиперпараметры хранятся в self.config\n",
    "    return MyLoss                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание агента\n",
    "config = {\n",
    "    \"env\": env,\n",
    "    \"network\": CartpoleNN,\n",
    "    \"buffer_size\": 10^4,\n",
    "    \"optimizer\": Adam,\n",
    "    \"target_update_frequency\": 100,\n",
    "    \"hp\": 42\n",
    "}\n",
    "\n",
    "Agent = Runner()\n",
    "Agent = Replay(Agent)\n",
    "Agent = DQN(Agent)                    # неявно создаётся голова нейросетки, оптимизатор, пайплайн обучения нейросети...\n",
    "Agent = Target(Agent)\n",
    "Agent = eGreedy(Agent)\n",
    "Agent = MyLoss(Agent)\n",
    "\n",
    "agent = Agent(config)\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки (причины появления данного файла):\n",
    "\n",
    "0.1) \"неявное\" создание модулей.\n",
    "\n",
    "0.2) гиперпараметры модулей слились в одну кучу. Непонятно, к какому модулю какой гиперпараметр относится.\n",
    "\n",
    "1) очевидно, сделать в системе два DQN или два оптимизируемых функционала можно только через одно место.\n",
    "\n",
    "2) всё лежит в одном объекте и рискует перезаписать переменные предыдущих агентов\n",
    "\n",
    "3) пользователю может быть неочевидно, в каком порядке нужно перечислять модули"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №1.1. Альтернативный старый вариант"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(+) минимальное количество строчек кода\n",
    "(+) аккуратный синтаксис\n",
    "(-) основные проблемы 1-3 предыдущего варианта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "# всё ещё отсутствуют!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это пишет пользователь, придумавший свою лосс-функцию\n",
    "def MyLoss(parclass, hp=42):                       # соглашение: модуль может быть унаследован от произвольного класса\n",
    "    class MyLoss(parclass):\n",
    "        def loss(self, prediction, truth):\n",
    "            return hp\n",
    "    return MyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent = Runner(env=env)\n",
    "Agent = Replay(Agent, buffer_size=10^4)\n",
    "Agent = Network(Agent, network=CartpoleNN, optimizer=Adam)\n",
    "Agent = DQN(Agent)\n",
    "Agent = Target(Agent, target_update_frequency=100)\n",
    "Agent = eGreedy(Agent)\n",
    "Agent = MyLoss(Agent, hp=42)\n",
    "\n",
    "agent = Agent()\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №затыка. Пытаемся разбить на отдельные блоки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы решить проблему, что все модули стакаются в один объект, сделаем так:\n",
    "* модули получают в качестве гиперпараметров ссылки на другие необходимые модули (зависимости от других модулей придётся указывать явно)\n",
    "* от модулей всё ещё можно получать новые модификации путём наследования\n",
    "\n",
    "Пробуем лобовой подход:\n",
    "* гиперпараметры передаём при создании класса\n",
    "* ссылки на другие модули (уже объекты классов) передаём в конструктор модуля\n",
    "\n",
    "Соответственно, перед передачей ссылки нужно создать модуль."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "# всё ещё отсутствуют!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это пишет пользователь, придумавший свою лосс-функцию\n",
    "def MyLoss(parclass, hp=42):\n",
    "    class MyLoss(parclass):\n",
    "        def loss(self, prediction, truth):\n",
    "            return hp\n",
    "    return MyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в нашем примере пусть будет 4 модуля: runner, replay, network, dqn\n",
    "runner = Runner(env=env)()\n",
    "replay = Replay(buffer_size=10^4)(runner)      # buffer_size - гиперпараметр, ссылка на модуль runner идёт в конструктор\n",
    "network = Network(optimizer=Adam)()\n",
    "\n",
    "dqn = DQN()                                    # пока это класс\n",
    "dqn = Target(dqn, target_update_frequency=100) # улучшаем класс\n",
    "dqn = MyLoss(dqn, hp=42)                       # ещё улучшаем\n",
    "dqn = dqn(replay, network)                     # вызываем конструктор, создавая модуль и передавая необходимые ссылки\n",
    "eGreedy = ?!?                                  # а вот и засада\n",
    "\n",
    "runner.run(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В чём здесь проблема: в рекурсивной зависимости. Runner-у должно быть принипиально пофиг, сколько ещё модулей есть в системе. При этом ему нужна стратегия (метод def act(self, s)), которым мы, собственно, играем в игры. От Runner-а зависит реплей буффер, от буффера DQN. Но затем нужно подцепить в Runner ссылку на DQN (а точнее даже как-то на eGreedy)...\n",
    "\n",
    "eGreedy можно в рамках концепции полагать или наследником DQN, или наследником Runner-а, но проблему это не решает. В первом случае непонятно, как обновить метод runner.act уже после создания runner-а (писать runner.act = dqn.act, очевидно, отвратительнейший вариант, и в более сложных алгоритмах подобные рекурсивные связи - частое явление (например, Twin DQN)). Во втором случае runner уже создан, и как унаследоваться от класса и элегантно \"обновить\" его экземпляр непонятно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №2. Класс System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы решить проблему, делаем так:\n",
    "* все модули наследуются от базового класса RLmodule\n",
    "* класс System компонует модули в одну рабочую систему\n",
    "* от модулей всё ещё можно получать новые модификации путём наследования, зависимости от других модулей придётся указывать явно.\n",
    "\n",
    "Тогда необходимо предоставить интерфейс связывания модулей.\n",
    "\n",
    "Первый вариант получается немного упоротым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "class System:\n",
    "    def __init__(self):\n",
    "        self.modules = []\n",
    "    \n",
    "    def add(self, module):\n",
    "        # module - класс (!), унаследованный от RLmodule\n",
    "        \n",
    "        # добавляет модуль в список модулей\n",
    "        self.modules += [module]\n",
    "        \n",
    "        # возвращает ID\n",
    "        return len(self.modules)\n",
    "    \n",
    "    def update(self, module_id, update, args):\n",
    "        # берёт модуль с ID=module (это класс), наследует от него update и кладёт по тому же ID.\n",
    "        \n",
    "    def create(self):\n",
    "        # инициализирует (вызывает конструкторы) все модули\n",
    "\n",
    "class RLmodule:\n",
    "    def __init__(self, system)\n",
    "        self.system = system\n",
    "        \n",
    "# зачем: способ обращения к другому модулю будет выглядеть тогда как-то так:\n",
    "# на примере вызова реплей-буффера из DQN:\n",
    "self.system[self.replay].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это пишет пользователь, придумавший свою лосс-функцию\n",
    "def MyLoss(parclass=RLmodule, hp=42):       # соглашение: модуль должен быть унаследован от RLmodule или производного\n",
    "    class MyLoss(parclass):\n",
    "        def loss(self, prediction, truth):\n",
    "            return hp\n",
    "    return MyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = System()\n",
    "runner = system.add(Runner(env=env))                                # Runner возвращает класс, унаследованный от RLmodule\n",
    "replay = system.add(Replay(runner=runner, buffer_size=10^4))        # Replay тоже, но ещё он запоминает ID runner-а\n",
    "network = system.add(Network(optimizer=Adam))                       \n",
    "dqn = system.add(DQN(replay, network))                              # DQN запоминает ID модулей replay, network в системе\n",
    "dqn = system.update(dqn, Target, {\"target_update_frequency\": 100})  # system.update наследует Target от DQN\n",
    "dqn = system.update(dqn, MyLoss, {\"hp\": 42})                        # тоже самое\n",
    "runner = system.update(runner, eGreedy, {\"greedy_agent\": dqn})      # тоже самое, но eGreedy теперь ещё нужно подсоединитсья к dqn\n",
    "\n",
    "agent = system.create()\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(+) проблемы решены\n",
    "\n",
    "(-) system.add и system.update повсюду, причём нужно думать, что где ставить\n",
    "\n",
    "(-) разный синтаксис передачи гиперпараметров и подсоединений\n",
    "\n",
    "(-) непонятно, где подсоединяются модули, а где гиперпараметры\n",
    "\n",
    "(-) очень мутно и тяжеловесно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №3. Процедура сборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "class System:\n",
    "    def create(self, modules):\n",
    "        # см. вызов в примере далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это пишет пользователь, придумавший свою лосс-функцию\n",
    "def MyLoss(parclass, hp=42):\n",
    "    class MyLoss(parclass):\n",
    "        def loss(self, prediction, truth):\n",
    "            return hp\n",
    "    return MyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(env=env)\n",
    "replay = Replay(buffer_size=10^4)\n",
    "network = Network(optimizer=\"Adam\")\n",
    "dqn = DQN()\n",
    "dqn = Target(dqn, target_update_frequency=100)\n",
    "dqn = MyLoss(dqn, hp=42)\n",
    "runner = eGreedy(runner)\n",
    "\n",
    "dqn = dqn()\n",
    "runner = runner()\n",
    "replay = replay()\n",
    "network = network()\n",
    "\n",
    "agent = System().create(\n",
    "    (runner, {\"dqn\": dqn})\n",
    "    (replay, {\"runner\": runner}),\n",
    "    (network, {}),\n",
    "    (dqn, {\"runner\": runner, \"network\": network})\n",
    ")\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути, System делает сейчас что-то вроде такого: \\\n",
    "runner.dqn = dqn \\\n",
    "replay.runner = runner \\\n",
    "dqn.runner = runner \\\n",
    "dqn.network = network\n",
    "\n",
    "Это порешало многие проблемы, но процедура инициализации очень громоздкая.\n",
    "\n",
    "(-) вызов System.create костылющий\n",
    "\n",
    "(-) сначала блок создания классов, потом блок создания объектов, потом большой вызов System..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №4. Сборка по связям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "class System:\n",
    "    def __init__(self, **kwargs):\n",
    "        # см. вызов в примере далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пользователю становится тяжелее...\n",
    "def MyLoss(name, hp=42):\n",
    "    def MyLoss(parclass):\n",
    "        class MyLoss(parclass):\n",
    "            def __init__(self, system):\n",
    "                super().__init__(self, system, name)   # нужно указывать явно, чтобы передать name...\n",
    "            \n",
    "            def loss(self, prediction, truth):\n",
    "                return hp\n",
    "        return MyLoss\n",
    "    return MyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = System(\n",
    "    Runner(\"runner\", env=env),\n",
    "    eGreedy(\"runner\", dqn=\"dqn\"),\n",
    "    Replay(\"replay\", runner=\"runner\", buffer_size=10^4),\n",
    "    Network(\"network\", optimizer=Adam),\n",
    "    DQN(\"dqn\", runner=\"runner\", replay=\"replay\"),\n",
    "    Target(\"dqn\", target_update_frequency=100),\n",
    "    MyLoss(\"dqn\", hp=42)\n",
    ")\n",
    "\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делает System: для каждого аргумента наследует очередной элемент списка от предыдущего, если их имена совпадают, заменяет поля-токены соответственно именам, поданным в System.\n",
    "\n",
    "(-) отвратительнейшее оформление нового модуля (функция, возвращающая функцию, возвращающую класс + явный конструктор)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариант №5. Искусственное наследование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не наследуем модули одни от других. Все модули просто унаследованы напрямую от RLmodule, и искусственный механизм наследования как-то (?) запихнут в System и RLmodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СКРЫТЫЕ ВНУТРЕННОСТИ АРХИТЕКТУРЫ:\n",
    "class RLmodule:\n",
    "    def __init__(self, name):\n",
    "        self._name = name\n",
    "\n",
    "class System:\n",
    "    def __init__(self, **kwargs):\n",
    "        # см. вызов в примере далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLoss(RLmodule):\n",
    "    def __init__(self, name, hp=42):\n",
    "        super().__init__(self, name)\n",
    "        self.hp = hp\n",
    "\n",
    "    def loss(self, prediction, truth):\n",
    "        return self.hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = System(\n",
    "    Runner(\"runner\", env=env),\n",
    "    Replay(\"replay\", runner=\"runner\", buffer_size=10^4),\n",
    "    Network(\"network\", optimizer=Adam),\n",
    "    DQN(\"dqn\", runner=\"runner\", replay=\"replay\"),\n",
    "    Target(\"dqn\", target_update_frequency=100),\n",
    "    MyLoss(\"dqn\", hp=42),\n",
    "    eGreedy(\"runner\", dqn=\"dqn\"),\n",
    ")\n",
    "\n",
    "agent.learn(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(+) получилась какая-то приемлемая внешность фреймворка\n",
    "\n",
    "(-) без наследования сами модули будут иметь кучу костылей. В частности, у них не будет (прямого) доступа к полям \"предков\", и это надо будет костылить в искусственном наследовании. И к тому, как его делать, тоже много вопросов :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫВОДЫ:** один вариант хуже другого."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
